{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dropout.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9ge4gJ_BJfBo"},"source":["# 1. Introduction to Dropout\n","\n","1.1 Why we need Dropout?\n","> In many machine learning model training, it involves many parameters to learn while the number of training instances are limited. The trained model under such a situation is likely to be overfitted. Especially in deep learning, the training loss is small and the training accuracy is high while the validation/testing loss is very high and the model cannot make good predictions on the validation/testing set. \n","\n","> A common way to mitigate the overfitting in ML is the idea of ensemble. However, training and testing multiple models are computationally expensive.\n","\n","> Therefore, Dropout comes to address the above mentioned two issues: overfitting and costly computation. Dropout is similar to regularization to some extent.\n","\n","1.2 What is Dropout?\n","\n","> In a peper entitled \"Improving neural networks by preventing co-adaptation of feature detectors\" by Hinton in 2012, Dropout was proposed. When a complex feedforward network is trained on a small training set, it's likely to overfit. To avoid overfitting, we can remove some feature detectors (e.g., neurons in hidden layers) to prevent their co-adaption. Note that co-adaption means one neuron can play a role only when some other neurons become effective. \n","\n","> The idea of Dropout is simple. During the forward pass, we remove some nerons with a probability of $p$ at random. Such a operation can make the model become \"thin\" and \"generic\", since every neuron is not relying on others too much (see Figure below).\n","\n","![dropout](https://drive.google.com/uc?id=1qw-nrqNkORUFs3Xk2W0c5UaeOMo4fH2m)"]},{"cell_type":"markdown","metadata":{"id":"nuZQ689WLUMq"},"source":["# 2. Dropout Mechanism\n","\n","2.1 Algorithm of Dropout\n","> (Drop some neurons (e.g. dashed lines in the Figure, make them ineffective) at random with the probability of $p$. The output neurons remain unchanged. \n","\n","> (2) For each batch training, do forward pass operations to obtain the loss for backpropagation to update parameters (w, b) via gradient descent. After this, we recover the deleted neurons to the original network. Parameters associated with these undropped neurons are updated. Parameters associated with these dropped ones are stored and remain the same as before deletion.\n","\n","> (3) Repeat the above steps (1) and (2).\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?id=1GcEcf2MmIUSoFPX_OgrGlDFn9cKfXYni' />\n","<figcaption>Figure: Dropout in a simple fully connected feedforward NN.</figcaption></center>\n","</figure>\n","\n","2.2 Application of Dropout in NN\n","How can we implement this Dropout in NN? Let's discuss it from the perspective of the math involved.\n","\n",">（1）Training period\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?id=11JNbe99uuoUGlO-1fF2HIJagxahrdn3i' />\n","<figcaption>Figure: Comparison of standard NN and NN with Dropout.</figcaption></center>\n","</figure>\n","\n",">> * forward passing without Dropout:\n","\n","$z_i^{(l+1)} = W_i^{(l+1)}+b_i^{(l+1)}$,\n","\n","$y_i^{(l+1)} = f(z_i^{(l+1)})$\n","\n",">> * forward passing with Dropout:\n","\n","$r_j^{(l)} \\sim Bernoulli(p)$\n","\n","$\\tilde{y}^{(l)} = r^{(l)}*y^{(l)}$\n","\n","$z_i^{(l+1)} = W_i^{(l+1)}\\tilde{y}^l+b_i^{(l+1)}$,\n","\n","$y_i^{(l+1)} = f(z_i^{(l+1)})$\n","\n","Bernoulli function above is to generate a random binary vector. Dropping neurons in code means that the activation functions in these neurons make the output become 0. For example, in a layer with 1000 neurons, the outputs after the activation functions are $y_1, y_2, \\cdots, y_{1000}$. If we choose the dropout rate to 0.4, then approximately 400 neurons' outputs are 0.\n","\n","> (2) Testing period\n","\n","Every parameter (W, b) needs to multiply $p$: $w_{test}^{(l)} = pW^{(l)}$. \n","\n",">> We cannot drop some neurons at test time. Because we will have unstable results for the same test data where it sometimes outputs a while outputing b at other times. This is not acceptable for end-users. One solution is to multiply $p$ for every parameter, which can make training and testing consistent. For example, the output of a neuron is $x$. If it's not dropped, it participates the training with the probability of $p$. The expected output is: $p*x + (1-p)*0 = px$. Therefore, we multiply $p$ for every parameter to obtain the same expected output as training.\n","\n","<figure>\n","<center>\n","<img src='https://drive.google.com/uc?id=1QJXSeHaauFI7sCxhyiYja05ReZ7uYPNR' />\n","<figcaption>Figure: Dropout at test time.</figcaption></center>\n","</figure>\n","\n","\n","# 3. Why Dropout can avoid overfitting?\n","\n","> (1) averaging. When we drop different sets of neurons, it’s equivalent to training different neural networks (as ensemble). So, the dropout procedure is like averaging the effects of large number of different networks. The different networks will overfit in different ways, so the net effect of dropout will be to reduce overfitting. Also, these networks all share weights i.e. we aren’t optimizing weights separately for these networks. (tip: so basically every network gets trained very rarely.) But, it works. It serves its purpose of regularization.\n","\n","> (2) preventing co-adaptation. In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data. We hypothesize that for each hidden unit, dropout prevents co-adaptation by making the presence of other hidden units unreliable. Therefore, a hidden unit cannot rely on other specific units to correct its mistakes.\n","\n","> (3) A motivation for dropout comes from a theory of the role of sex in evolution (Livnat et al., 2010). Please refer to the paper \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting.\""]},{"cell_type":"markdown","metadata":{"id":"2_XJMWxoQjvU"},"source":["# 4. Dropout in Pytorch\n","\n","In this section, we will \n","\n","(1) implement Dropout using Pytorch and \n","\n","(2) apply the build-in Dropout in Pytorch for a real application.\n","\n","All our implementations are based on PyTorch. The model training is on GPU and all other tasks are on CPU (you can run this notebook even you don't have GPU). To switch between GPU/CPU, you can add/remove `.cuda()` in the code."]},{"cell_type":"code","metadata":{"id":"KJAyw4gOQ2F_"},"source":["import numpy as np\n","import pandas as pd\n","import time\n","\n","import torch\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","import torch.nn as nn \n","import torch.optim as optim\n","from torch.utils.data.sampler import SubsetRandomSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OiZmZ_Ug0fCR"},"source":["# 4.1 Dropout implementation\n","\n","> We choose to multiply the dropout output by $\\frac{1}{1-p}$ where $p$ is the dropout rate (note this is different from $p$ in Keras, keep probability) to compensate for the dropped neurons during the training."]},{"cell_type":"code","metadata":{"id":"IE8Yqaw_1Gro"},"source":["class MyDropout(nn.Module):\n","\n","  def __init__(self,p=0.5):\n","    super(MyDropout,self).__init__()\n","    self.p = p\n","\n","    if self.p < 1:\n","      self.multiplier = 1.0/(1.0-p)\n","    else:\n","      self.multiplier = 0.0 # to avoid division by zero error\n","\n","  def forward(self, x):\n","    if not self.training:\n","      return x\n","\n","    # we have `input.shape` numbers of Bernoulli(1-p) samples to keep\n","    selected = torch.Tensor(x.shape).uniform_(0,1) > self.p\n","\n","    # to support both CPU and GPU\n","    if x.is_cuda:\n","      selected = Variable(selected.type(torch.cuda.FloatTensor), requires_grad=False)\n","    else:\n","      selected = Variable(selected.type(torch.FloatTensor), requires_grad=False)\n","\n","    return torch.mul(selected,x)*self.multiplier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZD72kl6n2uzV"},"source":["## 4.2 Dropout in Fully Connected FeedForward Networks\n","\n","> (1) we load the MNIST data from torchvision\n","\n","> (2) we build a multilayer perceptron (MLP) and show our implementation of Dropout is correct. "]},{"cell_type":"code","metadata":{"id":"didbTZtE3Sa3","executionInfo":{"status":"ok","timestamp":1595458835094,"user_tz":240,"elapsed":806,"user":{"displayName":"Kunpeng Zhang","photoUrl":"","userId":"09274433828486852799"}},"outputId":"94ee7f2d-9942-40d5-a755-db1a66627054","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0],[1])])\n","\n","trainset = datasets.MNIST(root='data/',train=True,download=True,transform=transform)\n","testset = datasets.MNIST(root='data/',train=False,transform=transform)\n","print(len(trainset),len(testset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["60000 10000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"naTvh0C64Bg2"},"source":["# define a fully-connected feedforward network\n","class FFNN(nn.Module):\n","\n","  def __init__(self,hidden_layers=[800,800],droprates=[0,0]):\n","    super(FFNN,self).__init__()\n","    self.model = nn.Sequential()\n","    self.model.add_module('dropout0',MyDropout(p=droprates[0]))\n","    self.model.add_module('input',nn.Linear(28*28,hidden_layers[0]))\n","    self.model.add_module('tanh',nn.Tanh())\n","\n","    for i,d in enumerate(hidden_layers[:-1]):\n","      self.model.add_module('dropout_hidden'+str(i+1),MyDropout(p=droprates[1]))\n","      self.model.add_module('hidden'+str(i+1),nn.Linear(hidden_layers[i],hidden_layers[i+1]))\n","      self.model.add_module('tanh_hidden'+str(i+1),nn.Tanh())\n","      \n","    self.model.add_module('final',nn.Linear(hidden_layers[-1],10))\n","\n","  def forward(self,x):\n","    x = x.view(x.shape[0],28*28)\n","    x = self.model(x)\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rizYgJ-H6C1H"},"source":["# # define a classifier for training\n","\n","class FFNNClassifier:\n","  \n","  def __init__(self,hidden_layers=[800,800],droprates=[0,0],batch_size=128,max_epoch=10,lr=0.1,momentum=0):\n","    self.hidden_layers = hidden_layers\n","    self.droprates = droprates\n","    self.batch_size = batch_size\n","    self.max_epoch = max_epoch\n","    self.model = FFNN(hidden_layers=hidden_layers,droprates=droprates)\n","    self.model.cuda()\n","    self.criterion = nn.CrossEntropyLoss().cuda()\n","    self.optimizer = optim.SGD(self.model.parameters(),lr=lr,momentum=momentum)\n","    self.loss = []\n","    self.test_accuracy = []\n","    self.test_error = []\n","    #print(self.model)\n","\n","  def fit(self,trainset,testset,verbose=True):\n","    trainloader = torch.utils.data.DataLoader(trainset,batch_size=self.batch_size,shuffle=True)\n","    testloader = torch.utils.data.DataLoader(testset,batch_size=len(testset),shuffle=True)\n","    X_test,y_test = next(iter(testloader))\n","    X_test = X_test.cuda()\n","\n","    for epoch in range(self.max_epoch):\n","      running_loss = 0\n","      for i,data in enumerate(trainloader):\n","        inputs,labels = data\n","        inputs,labels = Variable(inputs).cuda(),Variable(labels).cuda()\n","        \n","        self.optimizer.zero_grad()\n","        outputs = self.model(inputs)\n","        loss = self.criterion(outputs,labels)\n","        loss.backward()\n","        self.optimizer.step()\n","        \n","        running_loss += loss.item()\n","      self.loss.append(running_loss / len(trainloader))\n","\n","      if verbose:\n","        print('Epoch {} loss: {}'.format(epoch+1, self.loss[-1]))\n","      \n","      y_test_pred = self.predict(X_test).cpu()\n","      self.test_accuracy.append(np.mean(y_test_pred.data.numpy() == y_test.data.numpy()))\n","      self.test_error.append(int(len(testset)*(1-self.test_accuracy[-1])))\n","\n","      if verbose:\n","        print('# Misclassified: {}, test accuracy: {}'.format(self.test_error[-1],self.test_accuracy[-1]))\n","      \n","    return self\n","\n","  def predict(self,x):\n","    model = self.model.eval()\n","    outputs = model(Variable(x))\n","    _,pred = torch.max(outputs.data,1)\n","    model = self.model.train()\n","\n","    return pred\n","\n","  def __str__(self):\n","    return 'Hidden layers: {}, dropout rates: {}'.format(self.hidden_layers, self.droprates)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RBvqcr3QeA-N"},"source":["# run various testing experiments using different settings\n","hidden_layers = [256,256]\n","max_epoch = 10\n","\n","# define networks\n","ffnn = [FFNNClassifier(hidden_layers=hidden_layers, droprates=[0,0]),\n","        FFNNClassifier(hidden_layers=hidden_layers, droprates=[0,0.5]),\n","        FFNNClassifier(hidden_layers=hidden_layers, droprates=[0.2,0.5])]\n","\n","# training\n","for i in range(len(ffnn)):\n","  m = ffnn[i]\n","  print('Processing model {}\\n====='.format(i+1))\n","  m.fit(trainset,testset) \n","\n","# save torch models\n","for ind,m in enumerate(ffnn):\n","  torch.save(m.model,'model_'+str(ind)+'.pth')"],"execution_count":null,"outputs":[]}]}