<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">

<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
<title>BUDT 758B: Big Data and AI for business (Fall 2020)</title>
</head>

<body style="background-color: white">



<div style="max-width: 900px; background-color: #FAFDFD; padding: 0.5cm; margin: 0.5cm auto;"> 

    <h2><font color="#990033">BUDT 758B: Big Data and AI for business (Fall 2020)</font></h2><br><pre>
    Department of Decison, Operations &amp; Information Technologies</a>
    <a href="http://www.umd.edu/">University of Maryland, College park</a>

    Instructor:  <a href="#">Kunpeng Zhang</a>&nbsp; (<a href="mailto:kpzhang@umd.edu">kpzhang@umd.edu</a>)

    <a href="BUDT758B_syllabus.pdf"><b>Course Syllabus</b></a>

    Lecture-Discussion: Tuesday, Thursday (Zoom ID: 3206804434)
                        2:00--3:15, Online (section 01)
                        3:30--4:45, Online (section 02)
                        5:00--6:15, Online (section 03)
           Office hour: Monday (1:00 - 3:00pm), Online)

    TA: 
    Session 0501 - Mingwei Sun (ms1991@umd.edu): Thursday 6:00 - 7:00PM: (Zoom ID: 395294617761)

    Session 0502 - Zhonghao Li (zhonghao.li@marylandsmith.umd.edu): Wednesday 3:00 - 4:00PM: (Zoom ID: 36811380544)

    Session 0503 - Adityta Solai (adithyasolai7@gmail.com): Friday 2:00 - 3:00PM: (Zoom ID: 37531538659)


</pre><hr><table>

<h3><font color="#990033">Recommended textbooks</font></h3>
<ul>
<li><a href="http://www.amazon.com/Mining-Massive-Datasets-Anand-Rajaraman/dp/1107015359">Mining of Massive Datasets</a></li>
<li><a href="http://www.amazon.com/Hadoop-Action-Chuck-Lam/dp/1935182196/ref=sr_sp-atf_title_1_1?s=books&ie=UTF8&qid=1376947382&sr=1-1&keywords=hadoop+in+action">Hadoop in Action</a></li>
<li><a href="http://www.amazon.com/Hadoop-Practice-Alex-Holmes/dp/1617290238/ref=sr_sp-atf_title_1_1?s=books&ie=UTF8&qid=1376947438&sr=1-1&keywords=hadoop+in+practice">Hadoop in Practice</a></li>
<li>Deep Learning, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville
    (<a href='https://github.com/HFTrader/DeepLearningBook/blob/master/DeepLearningBook.pdf'>DeepLearningBook.pdf</a>)</li>
</ul>

<hr>

<h3><font color="#990033">Prerequisites</font></h3>
<ul>
<li>Data Mining Algorithms: supervised/unsupervised learning techniques</li>
<li>Basic programming - Java or scripting languages: Python/Perl</li>
<li>Database query language: SQL</li>
<li>Algorithm and data structures</li>
<li>Probability, Statistics, and Matrices</li>
</ul>
<hr>

<h3><font color="#990033">Useful Documentation and Data</font></h3>

<ul>

<li>Apache Hadoop Online Documentation: <a href="http://hadoop.apache.org/docs/current/">http://hadoop.apache.org/docs/current/</a></li>
<li>Apache Hbase: <a href="http://hbase.apache.org/">http://hbase.apache.org/</a></li>
<li>Apache Hive: <a href="http://hive.apache.org/">http://hive.apache.org/</a></li>
<li>Apache Mahout - Scalable Machine Learning Library: <a href="http://mahout.apache.org/">http://mahout.apache.org/</a></li>
<li>Apache Spark: <a href="https://spark.apache.org/">https://spark.apache.org/</a></li>
<li>Deep Learning: <a href="http://deeplearning4j.org/">http://deeplearning4j.org/</a></li>
<li>Stanford Large Network Dataset Collection: <a href="http://snap.stanford.edu/data/">http://snap.stanford.edu/data/</a></li>
<li>Some Time Series Data Collections: <a href="http://www-bcf.usc.edu/~liu32/data.htm">http://www-bcf.usc.edu/~liu32/data.htm</a></li>
<li>Mahout Testing Data: <a href="https://cwiki.apache.org/confluence/display/MAHOUT/Collections">https://cwiki.apache.org/confluence/display/MAHOUT/Collections</a></li>
<li>Collecting Online Data (Facebook Graph API and Twitter Stream API)<br><a href="https://developers.facebook.com/docs/reference/api/">Facebook Graph API</a><br><a href="https://dev.twitter.com/docs/streaming-apis">Twitter Stream API</a><br><a href="http://www.quandl.com/">Quandl: financial, economic and social datasets</a></li>
<li><a href="http://kevinchai.net/datasets">http://kevinchai.net/datasets</a></li>
<li>Kaggle: <a href="http://www.kaggle.com/competitions">http://www.kaggle.com/competitions</a></li>
</ul>

<hr>


<h3><font color="#990033">Class Lectures</font></h3>

<font color="#990033">1<sup>st</sup> Week Introduction to Big Data</font><br>Lecture Notes: <a href="./lecture_notes/1_1_introduction.pdf">Introduction</a>

<p>Welcome to the BUDT 758B: Big Data Analytics. Big data is newly emerging topics and becomes popular in various domains, including business, computer science, biological science, climate science, and others. The data generated by users grows rapidly, which needs us to find scalable methods and platforms to collect, store, and analyze them in order to provide informed decisions. </p>

<p>This is a graduate-level class. Data mining and some basic math (probability, statistics) knowledge are required. There are many online resources which can be used to help you understand this area. No textbooks are specified. During the first week, we will discuss: (1) Syllabus; (2) The Introduction to big data; (3) Examples that are successfully using big data techniques; (4) Business value of big data and AI.


<p>The required reading for the first week is the <a href="BUDT758B_syllabus.pdf">course syllabus</a>.</p>
<hr>

<font color="#990033">2<sup>nd</sup> Week Business Value and Internet of Things</font><br>Lecture Notes: <a href="./lecture_notes/1_2_businessValue.pdf">Business Value</a> | <a href="./lecture_notes/1_3_IoT.pdf">IoT</a><br>
<hr>

<font color="#990033">3<sup>rd</sup> Week Review of Machine Learning Algorithms</font><br>Lecture Notes: <a href="./lecture_notes/1_4_ML_review.pdf">ML Review</a><br>
<hr>

<font color="#990033">4,5,6,7<sup>th</sup> Week Deep Learning (1)</font><br>Lecture Notes: <a href="./lecture_notes/2_deeplearning_foundation.pdf">DL Foundation</a> | <a href="./lecture_notes/3_autoencoder.pdf">Autoencoder</a> | <a href="./lecture_notes/3_deeplearning_word2vec.pdf">Word2vec</a>
<p>Deep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high level abstractions in data. We will introduce basics of deep learning in this lecture, including how to build deep neural networks and perform parameter learning, autoencoder, and word2vec.<br/>
<br /><a href="./lecture_notes/758B-Lab-1.pdf">DL foundation lab</a> (<a href="./lecture_notes/758B-Lab-1-KZ.ipynb">Solution</a>)<br />
<a href="./lecture_notes/758B-Lab-2-Dropout.ipynb">Dropout lab</a><br />
<a href="./lecture_notes/758B-Lab-2-word2vec.py">Spark Lab</a>
<hr>

<font color="#990033">8,9<sup>th</sup> Week Deep Learning (2)</font><br>Lecture Notes: <a href="./lecture_notes/4_1_deeplearning_cnn_rnn.pdf">CNN, RNN</a>
<p>Deep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high level abstractions in data. We will introduce the variants of network architectures, such as CNN and RNN (LSTM).<br />
<a href="./lecture_notes/758B-Lab-2.pdf">Image classification</a> (<a href="./lecture_notes/758B-Lab-2-KZ.ipynb">Solution</a>)<br />
<a href="./lecture_notes/758B-Lab-3.pdf">Sentiment classification using LSTM</a> (<a href="#">Solution</a>)
<hr>


<font color="#990033">10<sup>th</sup> Week Architecture of Hadoop</font><br>Lecture Notes: <a href="./lecture_notes/5_hadoop.pdf">Hadoop</a>
<p>In this week, we will discuss the framework and the architecture of the Hadoop system. In addition, we will have an in-class lab of configuration and installation for a pseudo-distributed Hadoop system, mainly including how to configure the environment of Hadoop daemons, site-specific configuration of Hadoop daemons, how to start and stop Hadoop system, and how to monitor the Hadoop system. There are many online tutorials and documentation. Please read them in advance if you have time. Please bring your laptop to the class.</p>
<hr>

<font color="#990033">11<sup>th</sup> Week MapReduce Programming</font><br>Lecture Notes: <a href="./lecture_notes/6_mapreduce.pdf">MapReduce</a>
<p>In this week, we discuss how to write a MapReduce program and make it work in Hadoop. Three important components: Mapper, Reducer, and Driver classes. To make the MapReduce program more efficient, some operations such as Combiner, Partitioner, sorting are also introduced. To deal with different specific tasks, you can write your own Writable data types, your own inputformat and outputformat.</p>
<hr>


<font color="#990033">12<sup>th</sup> Week Hive, Impala, Pig, and Sqoop</font><br>Lecture Notes: <a href="./lecture_notes/7_Hive_Impala.pdf">Hive Impala</a>, <a href="./lecture_notes/8_Pig_Sqoop.pdf">Pig Sqoop</a>
<p>The Apache Hive data warehouse software facilitates querying and managing large datasets residing in distributed storage. Hive provides a mechanism to project structure onto this data and query the data using a SQL-like language called HiveQL. At the same time this language also allows traditional map/reduce programmers to plug in their custom mappers and reducers when it is inconvenient or inefficient to express this logic in HiveQL. This week discusses Hive, including introduction to Hive, Hive architecture, HiveQL, and installation and configuration, and similarly for Pig and Sqoop.

<hr>
<font color="#990033">13<sup>th</sup> Week Spark RDD</font><br>Lecture Notes: <a href="./lecture_notes/9_Spark.pdf">Spark RDD and DataFrame</a>
<p>Apache Spark is a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. In this week, we will cover the following materials: Introduction to Spark, working with RDD, building and running Spark applications.

<hr>
<font color="#990033">14, 15<sup>th</sup> Week Spark SQL/ML/Graphx</font><br>Lecture Notes: <a href="./lecture_notes/10_Spark.pdf">Spark ML</a><br /><a href="./lecture_notes/spark-lab.zip">Spark Lab</a>
<p>Apache Spark is a fast and general engine for big data processing, with built-in modules for streaming, SQL, machine learning and graph processing. In this week, we will cover the following materials: Spark SQL, MLlib, and GraphX.



<hr>

<!--font color="#990033">11<sup>th</sup> Week: Nov. 02 &amp; 04</font><br>Lecture Notes: <a href="./lecture_notes/10_network.pdf">PDF</a>
<p>There is much information to be gained by analyzing the large-scale data that is derived from social networks. The best-known example of a social network is the "friends" relation found on sites like Facebook. However, as we shall see there are many other sources of data that connect people or other entities. In this week, we will learn how to identify the importance of a node in a graph using PageRank, HITS, and topic-specific PageRank. In addition, we also discuss how to distribute PageRank using MapReduce.

<hr>
<hr>

<font color="#990033">8<sup>th</sup> Week: Mar. 13 &amp; 15</font><br>Lecture Notes: <a href="./lecture_notes/6_mba.pdf">PDF</a>
<p>We will discuss one of the major families of techniques for characterizing data: the discovery of frequent itemsets. We will introduce the "market-basket" model of data first. We then discuss the Apriori algorithm and various improvements to the basic Apriori idea. Finally, we also consider approximate algorithms that work faster but are not guaranteed to find all frequent itemsets. Exploiting prallelism through map-reduce is also one of tasks covered in this week.
<hr>

<font color="#990033">10<sup>th</sup> Week: Mar. 27 &amp; 29</font><br>Lecture Notes: <a href="./lecture_notes/7_clustering.pdf">PDF</a><br>Mahout example: <a href="./lecture_notes/mahout-kmeans.pdf">Kmeans</a><br>Perl file to view results: <a href="./lecture_notes/view.pl">view.pl</a>
<p>From this week, we will introduce some clustering techniques, such as K-means and Hierarchical clustering. All these are available in the scalable machine learning library: Mahout. We will discuss some strategies for parallelization using MapReduce framework. Some tweaks concerning distance measure, initial center choice and computation of new average centers will be explored, as well as the extimation of the number of clusters k.

<hr>
<font color="#990033">11<sup>th</sup> Week: Apr. 3 &amp; 5</font><br>Lecture Notes: <a href="./lecture_notes/8_CF.pdf">PDF</a>
<p> Collaborative Filtering (CF) is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with similar tastes to themselves. Collaborative filtering explores techniques for matching people with similar interests and making recommendations on this basis. In this week, we also discuss how to do item-based CF using Mahout under Hadoop for large scale of data.

<hr>
<font color="#990033">12<sup>th</sup> Week: Apr. 10 &amp; 12</font><br>Lecture Notes: <a href="./lecture_notes/9_LSH.pdf">PDF</a>
<p>A fundamental data-mining problem is to examine data for "similar" items. We begin by phrasing the problem of similarity as one of finding sets with a relatively large intersection. We show how the problem of finding textually similar documents can be turned into such a set problem by the technique known as "shingling". Then, we introduce a technique called "minhashing", which compresses large sets in such a way that we can still deduce the similarity of the underlying sets from their compressed versions. Another important problem that arises when we search for similar items of any kind is that there may be far too many pairs of items to test each pair for their degree of similarity, even if computing the similarity of any one pair can be made very easy. That concern motivates a technique called "locality-sensitive hashing", for focusing our search on pairs that are most likely to be similar.

<hr>
<font color="#990033">13<sup>th</sup> Week: Apr. 17 &amp; 19</font><br>Lecture Notes: <a href="./lecture_notes/10_LDA.pdf">PDF</a>
<p>Topic models are a suite of algorithms that uncover the hidden thematic structure in document collections. These algorithms help us develop new ways to search, browse and summarize large archives of texts. Latent Dirichlet Allocation (Blei et al, 2003) is a powerful learning algorithm for automatically and jointly clustering words into "topics" and documents into mixtures of topics. It has been successfully applied to model change in scientific fields over time (Griffiths and Steyvers, 2004; Hall, et al. 2008). This week we focus on LDA.

<hr>
<font color="#990033">12<sup>th</sup> Week: Nov. 09 &amp; 11</font><br>Lecture Notes: <a href="./lecture_notes/11_network.pdf">PDF</a>
<p>In this week, we continue discussing basic concepts of social network anlaysis, such as centralities, cohisive groups, and different types of techniques of community detection.

-->

</tbody></table>

</div>


</body></html>
